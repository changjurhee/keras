{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl7DHz3oX3fm"
   },
   "source": [
    "# 케라스 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-aVOc75X3fp"
   },
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "anIIyUo6X3fp"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LerBmDfX3fq"
   },
   "source": [
    "## 소개\n",
    "\n",
    "Keras 및 TensorFlow API 개념에 대한 첫 번째 소개 역할을 합니다.\n",
    "\n",
    "다음 방법을 배웁니다:\n",
    "\n",
    "- TensorFlow의 텐서, 변수 및 그라디언트\n",
    "- `Layer` 클래스를 서브 클래 싱하여 레이어 생성\n",
    "- 저수준 훈련 루프 작성\n",
    "- `add_loss()` 방법 을 통해 레이어에 의해 생성된 손실 추적\n",
    "- 낮은 수준의 훈련 루프에서 메트릭 추적\n",
    "- 컴파일된 실행 속도 `tf.function`\n",
    "- 훈련 또는 추론 모드에서 레이어 실행\n",
    "- 케라스 함수형 API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26s2g3y5X3fq"
   },
   "source": [
    "## 텐서\n",
    "\n",
    "TensorFlow는 미분 프로그래밍을 위한 인프라 계층입니다. 그 핵심은 NumPy와 마찬가지로 N차원 배열(텐서)을 조작하기 위한 프레임워크입니다.\n",
    "\n",
    "그러나 NumPy와 TensorFlow 사이에는 세 가지 주요 차이점이 있습니다:\n",
    "\n",
    "- TensorFlow는 GPU 및 TPU와 같은 하드웨어 가속기를 활용할 수 있습니다.\n",
    "- TensorFlow는 임의의 미분 가능한 텐서 표현식의 기울기를 자동으로 계산할 수 있습니다.\n",
    "- TensorFlow 계산은 단일 머신의 많은 장치와 많은 수의 머신(각각 여러 장치가 있을 수 있음)에 분산될 수 있습니다.\n",
    "\n",
    "TensorFlow의 핵심인 Tensor에 대해 살펴보겠습니다.\n",
    "\n",
    "다음은 상수 텐서입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ITRKupAlX3fq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5 2]\n",
      " [1 3]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[5, 2], \n",
    "                 [1, 3]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGGtPgJbX3fr"
   },
   "source": [
    "다음 `.numpy()` 을 호출하여 값을 NumPy 배열로 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IXhvc-AFX3fr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2],\n",
       "       [1, 3]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gvynfStX3fs"
   },
   "source": [
    "NumPy 배열과 매우 유사하며 다음과 같은 속성이 `dtype`, `shape`이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vErVzf8gX3fs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: <dtype: 'int32'>\n",
      "shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"dtype:\", x.dtype)\n",
    "print(\"shape:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-qbxS25X3fs"
   },
   "source": [
    "상수 텐서를 만드는 일반적인 방법은 `tf.ones` 와 `tf.zeros` 를 사용하는 것 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GiSGig9OX3fs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones(shape=(2, 1)))\n",
    "print(tf.zeros(shape=(2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTVVqXa8X3ft"
   },
   "source": [
    "난수로 상수 텐서를 만들 수도 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Qwc_c7QKX3ft"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.2206277 -2.1888824]\n",
      " [ 1.0467587  1.8161092]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4 6]\n",
      " [1 0]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)\n",
    "print(x)\n",
    "x = tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype=\"int32\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpoa_gilX3ft"
   },
   "source": [
    "## 변수\n",
    "\n",
    "변수는 변경 가능한 상태(예: 신경망의 가중치)를 저장하는 데 사용되는 특수 텐서입니다. `Variable` 일부 초기 값을 사용하여 생성 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "CcHM0IuLX3ft"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-0.6608966 ,  0.11948887],\n",
      "       [-1.2962437 ,  0.19271922]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "initial_value = tf.random.normal(shape=(2, 2))\n",
    "a = tf.Variable(initial_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cqEILkFX3ft"
   },
   "source": [
    ", 또는 다음  방법을 사용하여 `Variable` 의 값을 업데이트합니다 `.assign(value)`, `.assign_add(increment)`, or `.assign_sub(decrement)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "-0tmqUm7X3fu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[1., 1.],\n",
      "       [1., 1.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[2., 2.],\n",
      "       [2., 2.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "new_value = tf.ones(shape=(2, 2))\n",
    "a.assign(new_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j]\n",
    "\n",
    "print(a)        \n",
    "\n",
    "added_value = tf.ones(shape=(2, 2))\n",
    "a.assign_add(added_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j] + added_value[i, j]\n",
    "        \n",
    "print(a)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ4yXJ3rX3fu"
   },
   "source": [
    "## 텐서플로우의 계산\n",
    "\n",
    "NumPy를 사용한 적이 있다면 TensorFlow에서 수학을 수행하는 것이 매우 친숙해 보일 것입니다. 주요 차이점은 TensorFlow 코드가 GPU 및 TPU에서 실행될 수 있다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "5Yja4TNfX3fu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [5 6]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 4  9]\n",
      " [25 36]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1,2],\n",
    "                 [3,4]])\n",
    "b = tf.constant([[1,1],\n",
    "                 [2,2]])\n",
    "\n",
    "c = a + b\n",
    "print(c)\n",
    "d = tf.square(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52WPrn-AX3fu"
   },
   "source": [
    "## 그라디언트\n",
    "\n",
    "NumPy의 또 다른 큰 차이점은 미분 가능한 표현식의 기울기를 자동으로 검색할 수 있다는 것입니다.\n",
    "\n",
    "x 를 `tape.watch()` 열고 GradientTape를 통해 텐서를 \"감시\"하기 시작 하고 이 텐서를 입력으로 사용하여 미분 가능한 표현식을 작성하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "0uyb8JvLX3fu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 4.  8. 12.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1,3,5], dtype=\"float32\")\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)  \n",
    "    y = tf.square(x) + 2*x + 1    # x^2 + 2*x + 1  => 2*x + 2\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(dy_dx)  # [4, 8, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xegnClYhX3fu"
   },
   "source": [
    "기본적으로 변수는 자동으로 감시되므로 수동으로 변경할 필요가 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "KevKNQscX3fv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 4.  8. 12.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([1,3,5], dtype=\"float32\")\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x) + 2*x + 1  \n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(dy_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUOYvazQX3fv"
   },
   "source": [
    "## 케라스 레이어\n",
    "\n",
    "TensorFlow가 텐서, 변수 및 그라디언트를 처리하는 **미분 프로그래밍을 위한 인프라 계층**,\n",
    "인 반면, Keras는 계층, 모델, 옵티마이저, 손실 함수, 메트릭 등을 다루는 **딥 러닝을 위한 사용자 인터페이스** 입니다.\n",
    "\n",
    "Keras는 TensorFlow의 고급 API 역할을 합니다. Keras는 TensorFlow를 간단하고 생산적으로 만드는 것입니다.\n",
    "\n",
    "`Layer` 클래스는 Keras의 기본 추상화입니다 . A `Layer`는 상태(가중치)와 일부 계산(call 메서드에 정의됨)을 캡슐화합니다.\n",
    "\n",
    "간단한 레이어는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "D5cCbk4TX3fv"
   },
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        print('Linear.__init__()')\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print('Linear.call()')\n",
    "        return tf.matmul(inputs, self.w) + self.b   # (2,2)(2,4)+(4,) => (2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgKzvPrdX3fv"
   },
   "source": [
    "Python 함수와 매우 유사한 인스턴스 를 사용 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "VkzF-tnHX3fv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear.__init__()\n",
      "Linear.call()\n",
      "tf.Tensor(\n",
      "[[-0.11010438 -0.03130747  0.02373462 -0.06339031]\n",
      " [-0.11010438 -0.03130747  0.02373462 -0.06339031]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = Linear(units=4, input_dim=2)\n",
    "\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEmMB87NX3fv"
   },
   "source": [
    "에서 생성된 가중치 변수 는 다음 속성 `__init__`에서 자동으로 추적됩니다 `.weights` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "O0VNSv_NX3fw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[-0.04670297,  0.03103117,  0.0286093 , -0.07406551],\n",
      "       [-0.06340142, -0.06233865, -0.00487469,  0.01067521]],\n",
      "      dtype=float32)>, <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[-0.04670297,  0.03103117,  0.0286093 , -0.07406551],\n",
      "       [-0.06340142, -0.06233865, -0.00487469,  0.01067521]],\n",
      "      dtype=float32)> <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer.weights)\n",
    "print(linear_layer.w, linear_layer.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkUm3RoiX3fw"
   },
   "source": [
    "## 레이어 가중치 생성\n",
    "\n",
    "이 `self.add_weight()` 방법은 가중치 생성을 위한 바로 가기를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "DQGzatd4X3fw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.1119335   0.03524943 -0.00179264 -0.0752756 ]\n",
      " [ 0.1119335   0.03524943 -0.00179264 -0.0752756 ]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "linear_layer = Linear(4)\n",
    "\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N670i0veX3fw"
   },
   "source": [
    "## 레이어 그라디언트\n",
    "\n",
    "레이어를 내부에서 `GradientTape` 호출하여 레이어 가중치의 그라디언트를 자동으로 검색할 수 있습니다 . 이러한 그라디언트를 사용하여 수동으로 또는 최적화 개체를 사용하여 레이어의 가중치를 업데이트할 수 있습니다. 물론 필요한 경우 그라디언트를 사용하기 전에 수정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "-ARN3nlOX3fw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.336012840270996\n",
      "Step: 100 Loss: 2.277557373046875\n",
      "Step: 200 Loss: 2.1292476654052734\n",
      "Step: 300 Loss: 2.0603203773498535\n",
      "Step: 400 Loss: 1.9213716983795166\n",
      "Step: 500 Loss: 1.9477500915527344\n",
      "Step: 600 Loss: 1.7986361980438232\n",
      "Step: 700 Loss: 1.7613279819488525\n",
      "Step: 800 Loss: 1.6687450408935547\n",
      "Step: 900 Loss: 1.5995466709136963\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "linear_layer = Linear(10)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = linear_layer(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17rAFIWYX3fw"
   },
   "source": [
    "## 훈련 가능한 가중치와 훈련 불가능한 가중치\n",
    "\n",
    "레이어에 의해 생성된 가중치는 학습 가능하거나 학습 불가능할 수 있습니다. `trainable_weights`및 `non_trainable_weights` 각각 에 노출되어 있습니다. 다음은 훈련할 수 없는 가중치를 가진 레이어입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "f5HRbX_YX3fw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ComputeSum(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  \n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  \n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sVQeRBtX3fx"
   },
   "source": [
    "## 레이어를 소유한 레이어\n",
    "\n",
    "레이어를 재귀적으로 중첩하여 더 큰 계산 블록을 생성할 수 있습니다. 각 레이어는 하위 레이어의 가중치를 추적합니다(학습 가능 및 학습 불가능 모두)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "wZIP6CL8X3fx"
   },
   "outputs": [],
   "source": [
    "class MLP(keras.layers.Layer):\n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "\n",
    "assert len(mlp.weights) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhKPKJDqX3fx"
   },
   "source": [
    "위에서 수동으로 만든 MLP는 다음 기본 제공 옵션과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "NqtewCBFX3fx"
   },
   "outputs": [],
   "source": [
    "mlp = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "intro_to_keras_for_researchers",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
